import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.metrics import log_loss, roc_curve, auc

# ==========================================
# 0. ç¯å¢ƒè®¾ç½®ä¸æ•°æ®åŠ è½½
# ==========================================
# è®¾ç½®ä¸“ä¸šç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.family'] = 'sans-serif' # é˜²æ­¢å­—ä½“æŠ¥é”™
plt.rcParams['axes.unicode_minus'] = False

current_dir = os.path.dirname(os.path.abspath(__file__))
data_path = os.path.join(current_dir, '..', 'data', 'churn_data.csv')
image_dir = os.path.join(current_dir, '..', 'images')

if not os.path.exists(data_path):
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼")
    exit()

df = pd.read_csv(data_path)
X = df.drop('Churn', axis=1)
y = df['Churn']

# æ•°æ®æ ‡å‡†åŒ– (ç”»Lossæ›²çº¿å¿…é¡»è¦åšï¼)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print("ğŸ¨ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

# ==========================================
# å›¾è¡¨ 1: æŸå¤±å‡½æ•°æ”¶æ•›æ›²çº¿ (Loss Convergence Curve)
# ==========================================
# æ•°å­¦å«ä¹‰ï¼šå±•ç¤ºæ¢¯åº¦ä¸‹é™(Gradient Descent)å¦‚ä½•ä¸€æ­¥æ­¥æ‰¾åˆ°å±±è°·åº•éƒ¨çš„è¿‡ç¨‹
# æˆ‘ä»¬ä½¿ç”¨ SGDClassifier (éšæœºæ¢¯åº¦ä¸‹é™) å¹¶æ‰‹åŠ¨å¾ªç¯æ¥è®°å½• Loss

print("   1. æ­£åœ¨ç»˜åˆ¶æŸå¤±å‡½æ•°æ”¶æ•›æ›²çº¿...")
sgd_clf = SGDClassifier(loss='log_loss', learning_rate='constant', eta0=0.01, random_state=42, warm_start=True)

loss_history = []
epochs = 50

for epoch in range(epochs):
    # partial_fit å…è®¸æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥è®­ç»ƒ
    sgd_clf.partial_fit(X_train, y_train, classes=np.unique(y))
    # è®¡ç®—å½“å‰çš„ Log Loss (å¯¹æ•°æŸå¤±)
    y_pred_proba = sgd_clf.predict_proba(X_train)
    loss = log_loss(y_train, y_pred_proba)
    loss_history.append(loss)

plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), loss_history, color='#e74c3c', linewidth=2.5, marker='o', markersize=5)
plt.title('Loss Function Convergence (Gradient Descent)', fontsize=14, fontweight='bold')
plt.xlabel('Iterations (Epochs)', fontsize=12)
plt.ylabel('Log Loss', fontsize=12)
plt.grid(True, alpha=0.3)
plt.annotate('Start Optimization', xy=(1, loss_history[0]), xytext=(1, loss_history[0]+0.01),
             arrowprops=dict(facecolor='black', shrink=0.05))
plt.annotate('Converged', xy=(epochs, loss_history[-1]), xytext=(epochs-4, loss_history[-1]+0.01),
             arrowprops=dict(facecolor='black', shrink=0.05))

save_path = os.path.join(image_dir, 'loss_curve.png')
plt.savefig(save_path, dpi=300, bbox_inches='tight')
print(f"      -> å·²ä¿å­˜: {save_path}")

# ==========================================
# å›¾è¡¨ 2: ROC æ›²çº¿ (ROC Curve)
# ==========================================
# æ•°å­¦å«ä¹‰ï¼šè¡¡é‡æ¨¡å‹åŒºåˆ†æ­£è´Ÿæ ·æœ¬çš„èƒ½åŠ›ã€‚AUC (æ›²çº¿ä¸‹ç§¯åˆ†é¢ç§¯) è¶Šå¤§è¶Šå¥½ã€‚

print("   2. æ­£åœ¨ç»˜åˆ¶ ROC æ›²çº¿...")
# é‡æ–°è®­ç»ƒä¸€ä¸ªæ ‡å‡†çš„é€»è¾‘å›å½’ç”¨äºè¯„ä¼°
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
y_score = lr_model.predict_proba(X_test)[:, 1]

fpr, tpr, _ = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # çº¯éšæœºçŒœæµ‹çº¿
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Recall)')
plt.title('Receiver Operating Characteristic (ROC)', fontsize=14, fontweight='bold')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)

save_path = os.path.join(image_dir, 'roc_curve.png')
plt.savefig(save_path, dpi=300, bbox_inches='tight')
print(f"      -> å·²ä¿å­˜: {save_path}")

# ==========================================
# å›¾è¡¨ 3: ç‰¹å¾é‡è¦æ€§æ’åº (Feature Importance)
# ==========================================
# å•†ä¸šå«ä¹‰ï¼šå‘Šè¯‰è€æ¿å“ªä¸ªå› ç´ æœ€å½±å“å®¢æˆ·æµå¤±

print("   3. æ­£åœ¨ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾...")
# è·å–æƒé‡
weights = lr_model.coef_[0]
features = X.columns

# åˆ›å»º DataFrame å¹¶æ’åº
importance_df = pd.DataFrame({'Feature': features, 'Weight': weights})
importance_df = importance_df.sort_values(by='Weight', key=abs, ascending=False) # æŒ‰ç»å¯¹å€¼å¤§å°æ’åº

plt.figure(figsize=(10, 5))
# ç”¨é¢œè‰²åŒºåˆ†æ­£è´Ÿï¼šçº¢è‰²ä»£è¡¨æ­£ç›¸å…³(ä¿ƒè¿›æµå¤±)ï¼Œç»¿è‰²ä»£è¡¨è´Ÿç›¸å…³(æŠ‘åˆ¶æµå¤±)
colors = ['#e74c3c' if x > 0 else '#2ecc71' for x in importance_df['Weight']]
sns.barplot(x='Weight', y='Feature', data=importance_df, palette=colors)

plt.title('Feature Importance (Logistic Regression Coefficients)', fontsize=14, fontweight='bold')
plt.xlabel('Weight Impact (Standardized)', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.axvline(0, color='black', linewidth=0.8) # 0 è½´çº¿

save_path = os.path.join(image_dir, 'feature_importance.png')
plt.savefig(save_path, dpi=300, bbox_inches='tight')
print(f"      -> å·²ä¿å­˜: {save_path}")

print("\nâœ… æ‰€æœ‰å›¾è¡¨ç»˜åˆ¶å®Œæˆï¼è¯·å» images æ–‡ä»¶å¤¹æŸ¥çœ‹ã€‚")